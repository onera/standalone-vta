{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to use the compiler folder for the example of LeNet-5\n",
    "\n",
    "**Introduction to LeNet-5 : one of the first CNNs, useful for image recognition.**\n",
    "\n",
    "*Architecture :* \n",
    "- **Input image :** 32x32 pixels, 1 channel\n",
    "- **First convolutional layer C1 :** 6 convolutional filters of size 5x5, resulting in 6 feature maps of size 28x28 | ReLU activation\n",
    "- **Average pooling layer AP2 :** 2x2 kernel with stride = 2, resulting in feature maps of size 14x14 \n",
    "\n",
    "*How to use data_definition :*\n",
    "**Toolbox :** `numpy` tool is required for matrices manipulation : `conda install numpy` for a conda environment\n",
    "\n",
    "**Defining the size of the matrices :** \n",
    "The input image, represented by an input tensor matrix of size 32x32x1 (height x width x channels), goes through C1 to become an output tensor of size 28x28x6.\n",
    "In this case, to use the VTA and to do the convolution as a GEMM, we use 2D matrices by converting the input tensors with an Im2row method. We obtain an input matrix A (784x25) and a weight matrix B (25x6), whose multiplication results in an output matrix of size 784x6. This is done by ACETONE.\n",
    "The dimensions of the matrices are obtained using `tensor_matrix_converter.py` (no matrices are generated, only the dimensions):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /tmp_user/ldtis151h/lgeorgio/miniconda3/lib/python3.12/site-packages (2.2.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"IMPORTING NECESSARY FUNCTIONS\"\"\"\n",
    "\n",
    "%pip install numpy\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append('../compiler/data_definition')\n",
    "import tensor_matrix_converter\n",
    "import matrix_generator\n",
    "import matrix_split\n",
    "import matrix_multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input tensor: nc = 1, nh = 32, nw = 32 \n",
      "Output tensor: mc = 6, mh = 28, mw = 28 \n",
      "Kernel: fh = 5, fw = 5 \n",
      "Parameters: stride = (1, 1), pad = (0, 0) \n",
      "\n",
      "\n",
      "Input matrix: height = 784, width = 25 \n",
      "Weight matrix: height = 25, width = 6 \n",
      "Output matrix: height = 784, width = 6 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# To illustrate, let's generate the dimensions of the Input, Weight (post-Im2Row conversion), and Output matrices (after GeMM). \n",
    "\n",
    "# ----------------------\n",
    "# For that, the given dimensions of the Input tensor and Kernel are to be input :\n",
    "\n",
    "\"\"\"INPUT TENSOR\"\"\"\n",
    "input_channel = 1\n",
    "input_height = 32\n",
    "input_width = 32\n",
    "\n",
    "\"\"\"KERNEL\"\"\"\n",
    "kernel_channel = 6 # Number of filters\n",
    "kernel_height = 5\n",
    "kernel_width = 5\n",
    "\n",
    "\"\"\"Computation Parameters (for convolution)\"\"\"\n",
    "stride_height = 1\n",
    "stride_width = 1\n",
    "pad_height = 0\n",
    "pad_width = 0\n",
    "\n",
    "# Using `tensor_matrix_converter.py`, we can print the dimensions of the Output tensor (post-convolution) :\n",
    "\n",
    "\"\"\"OUTPUT TENSOR\"\"\"\n",
    "output_tensor_height, output_tensor_weight = tensor_matrix_converter.output_dimension(inp_dim=(input_height, input_width), \\\n",
    "                     wgt_dim=(kernel_height, kernel_width), \\\n",
    "                     stride=(stride_height, stride_width), \\\n",
    "                     padding=(pad_height, pad_width))\n",
    "\n",
    "# Then, we can print the dimensions of the Input and Weight matrices\n",
    "tensor_matrix_converter.im2row_matrix_dimension(nc=input_channel, nh=input_height, nw=input_width, \\\n",
    "                            mc=kernel_channel, mh=output_tensor_height, mw=output_tensor_weight, \\\n",
    "                            fh=kernel_height, fw=kernel_width, \\\n",
    "                            sh=stride_height, sw=stride_width, \\\n",
    "                            ph= pad_height, pw=pad_width)\n",
    "\n",
    "# Size of the input matrix\n",
    "inp_height = output_tensor_height * output_tensor_weight\n",
    "inp_width = input_channel * kernel_height * kernel_width\n",
    "# Size of the weight matrix\n",
    "wgt_height = inp_width\n",
    "wgt_width = kernel_channel\n",
    "# Size of the output matrix\n",
    "out_height = output_tensor_height * output_tensor_weight\n",
    "out_width = kernel_channel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Configuring the data generation :** \n",
    "i.e. whether to randomize the content of the matrices, to pad them, to use an activation function or not (ReLU), what type of files to write / print (JSON, binary), etc...\n",
    "For that, `user_configuration.py` is to be used (adjusting the parameters to True / False depending on the desired outcome).\n",
    "\n",
    "*For example, these parameters initialise the 784x25 input matrix A and 25x6 weight matrix B, so that their content is randomized.*\n",
    "\n",
    "```\n",
    "isInitRandom = True\n",
    "A_row = 784\n",
    "A_col = 25\n",
    "B_col = 6\n",
    "```\n",
    "\n",
    "*As the VTA requires square 16x16 matrices for multiplication ; a ReLU activation is then used :*\n",
    "\n",
    "```\n",
    "block_size = 16\n",
    "isSquare = True\n",
    "useReLU = True\n",
    "```\n",
    "\n",
    "*We want JSON files as outputs, so :*\n",
    "\n",
    "```\n",
    "doWriteBinaryFile = False\n",
    "doWriteJSON = True\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"MATRIX GENERATION\"\"\"\n",
    "# Matrices initialised with random value? (True / False)\n",
    "isInitRandom = True\n",
    "# If yes, random_bound limit the value range (int8 = [-128; 127] -> random_bound = 128)\n",
    "random_bound = 4\n",
    "\n",
    "\"\"\"COMPUTATION SPECIFICATION\"\"\"\n",
    "# The size of the square matrix multiplication (multiple two block_size square matrix together)\n",
    "block_size = 16 # VTA requirement\n",
    "\n",
    "# Use square matrix or not\n",
    "isSquare = True\n",
    "\n",
    "# Compute the non-padded matrix? (True / False)\n",
    "doMultiplyNonPadded = False\n",
    "\n",
    "# C matrix option\n",
    "# Reduction from int16 to int8: useClip (True / False)\n",
    "# => True: if x > 0: clip => max(127, x)\n",
    "# => False: Truncate the MSB\n",
    "useClip = False\n",
    "\n",
    "# Apply ReLU on the result\n",
    "useReLU = True\n",
    "\n",
    "\n",
    "\"\"\"PROMPTING AND DUMPING FILES FEATURES\"\"\"\n",
    "# Print the data (True / False)\n",
    "doPrint = True\n",
    "\n",
    "# Write matrices in binary files in OUTPUT dir (True / False)\n",
    "doWriteBinaryFile = False\n",
    "\n",
    "# Write a JSON file for CHISEL Compute in OUTPUT dir (True / False)\n",
    "doWriteJSON = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generating the data :**\n",
    "The program `main_matrix_generator.py` can generate .bin (binary) files for the *functional_simulator* and .json files for the *cycle_accurate_simulator* (using CHISEL). The files will be generated in the *compiler_output/* directory.\n",
    "\n",
    "\n",
    "It calls functions from several other programs : \n",
    "- `matrix_generator.py` : is used to generate the input and weight matrices (A size 784x25 and B size 25x6), according to `user_configuration.py` : the number of rows (height) and columns (width) of the matrix, the padding, if its content is to be randomized or filled with 0s. A and B are to be padded into 784x32 and 32x16 matrices for ease of splitting.\n",
    "- `matrix_split.py` : needed to split A and B into square 16x16 sub-matrices, as is required by the VTA (only takes matrices of this size for matrix multiplications).\n",
    "- `matrix_multiplication.py` : used for block matrix multiplication. A_block_i (16x16) and B_block_j (16x16) are multiplied to obtain an output sub-matrix (size 16x16 also). If the function ReLU is used, it is also applies to each of the values in the output matrices.\n",
    "- `json_generator.py` : translates the input data (and expected output) into a .json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Matrix ( 784 x 25 ) :\n",
      " [[-1 -2 -2 ...  1  1  2]\n",
      " [-3 -2  2 ...  1  1  0]\n",
      " [-3 -1 -3 ... -3  0  1]\n",
      " ...\n",
      " [-4  2  0 ... -4  0 -4]\n",
      " [ 1 -1  0 ...  0  1 -2]\n",
      " [-3  2 -1 ...  1 -2 -4]]\n",
      "Weight Matrix ( 25 x 6 ) :\n",
      " [[ 1 -2  0  0 -2  0]\n",
      " [ 2 -4 -1 -4 -1 -3]\n",
      " [-2 -4  1 -4 -2  0]\n",
      " [-2  1 -3 -2  1 -2]\n",
      " [ 1  0  2 -4  1  2]\n",
      " [-4 -4 -4  2  1  0]\n",
      " [-2 -1 -3 -1 -1 -3]\n",
      " [-3  0  0 -1 -1  0]\n",
      " [-2 -1  2 -3 -1  1]\n",
      " [ 0  2  0  1  1  2]\n",
      " [-1 -1  0 -3  0  1]\n",
      " [ 0 -2 -4 -3 -1 -3]\n",
      " [ 2  2  0  1 -4  2]\n",
      " [ 2 -4 -1  0 -2 -3]\n",
      " [-4  2 -3 -3  2 -2]\n",
      " [-1 -3 -1  1 -4 -3]\n",
      " [ 0 -4  2 -1 -2 -1]\n",
      " [ 0  2  1 -4 -2 -2]\n",
      " [-2  2 -1 -3  2 -1]\n",
      " [-3 -3 -1 -1 -2 -3]\n",
      " [ 1  1 -1 -2  0 -4]\n",
      " [-3 -2 -4 -4 -3  0]\n",
      " [-1 -1  0 -3 -2 -3]\n",
      " [-2  1  0  2 -1  1]\n",
      " [ 0 -3 -1 -3 -4  0]]\n"
     ]
    }
   ],
   "source": [
    "# ----------------------\n",
    "# Generate the matrix A and B with random values\n",
    "\n",
    "# Input Matrix A\n",
    "input_matrix = matrix_generator.matrix_creation(n_row=inp_height, n_col=inp_width, isInitRandom=isInitRandom, random_bound=random_bound)\n",
    "\n",
    "# Weight Matrix B\n",
    "weight_matrix = matrix_generator.matrix_creation(n_row=inp_width, n_col=wgt_width, isInitRandom=isInitRandom, random_bound=random_bound)\n",
    "\n",
    "print(\"Input Matrix (\",inp_height, \"x\", inp_width,\") :\\n\", input_matrix)\n",
    "print(\"Weight Matrix (\",inp_width, \"x\", wgt_width,\") :\\n\", weight_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padded Input Matrix ( 784 x 32 ) :\n",
      " [[-1 -2 -2 ...  0  0  0]\n",
      " [-3 -2  2 ...  0  0  0]\n",
      " [-3 -1 -3 ...  0  0  0]\n",
      " ...\n",
      " [-4  2  0 ...  0  0  0]\n",
      " [ 1 -1  0 ...  0  0  0]\n",
      " [-3  2 -1 ...  0  0  0]]\n",
      "Padded Weight Matrix ( 32 x 16 ) :\n",
      " [[ 1 -2  0  0 -2  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 2 -4 -1 -4 -1 -3  0  0  0  0  0  0  0  0  0  0]\n",
      " [-2 -4  1 -4 -2  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [-2  1 -3 -2  1 -2  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 1  0  2 -4  1  2  0  0  0  0  0  0  0  0  0  0]\n",
      " [-4 -4 -4  2  1  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [-2 -1 -3 -1 -1 -3  0  0  0  0  0  0  0  0  0  0]\n",
      " [-3  0  0 -1 -1  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [-2 -1  2 -3 -1  1  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  2  0  1  1  2  0  0  0  0  0  0  0  0  0  0]\n",
      " [-1 -1  0 -3  0  1  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 -2 -4 -3 -1 -3  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 2  2  0  1 -4  2  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 2 -4 -1  0 -2 -3  0  0  0  0  0  0  0  0  0  0]\n",
      " [-4  2 -3 -3  2 -2  0  0  0  0  0  0  0  0  0  0]\n",
      " [-1 -3 -1  1 -4 -3  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 -4  2 -1 -2 -1  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  2  1 -4 -2 -2  0  0  0  0  0  0  0  0  0  0]\n",
      " [-2  2 -1 -3  2 -1  0  0  0  0  0  0  0  0  0  0]\n",
      " [-3 -3 -1 -1 -2 -3  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 1  1 -1 -2  0 -4  0  0  0  0  0  0  0  0  0  0]\n",
      " [-3 -2 -4 -4 -3  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [-1 -1  0 -3 -2 -3  0  0  0  0  0  0  0  0  0  0]\n",
      " [-2  1  0  2 -1  1  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 -3 -1 -3 -4  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "# ----------------------\n",
    "# Padding the matrices so their dimensions can be divided by 16\n",
    "\n",
    "# Padded Input Matrix A\n",
    "input_matrix_padded = matrix_generator.matrix_padding(input_matrix)\n",
    "\n",
    "# Padded Weight Matrix B\n",
    "weight_matrix_padded = matrix_generator.matrix_padding(weight_matrix)\n",
    "\n",
    "print(\"Padded Input Matrix (\",input_matrix_padded.shape[0], \"x\", input_matrix_padded.shape[1],\") :\\n\", input_matrix_padded)\n",
    "print(\"Padded Weight Matrix (\",weight_matrix_padded.shape[0], \"x\", weight_matrix_padded.shape[1],\") :\\n\", weight_matrix_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Block Input Matrix ( 16 x 16 ) :\n",
      " [[-1 -2 -2 -3 -3 -3 -2 -4 -3  2 -4 -1  2 -3  1  2]\n",
      " [-3 -2  2  1 -1  1 -3 -2  0 -2 -3 -1  1  2 -3  1]\n",
      " [-3 -1 -3 -4 -4 -4 -3 -1  2 -4 -1  1 -1 -2 -4  1]\n",
      " [-3 -1 -1 -1 -4  2  1  0 -3 -2  1 -2  2  1 -4  0]\n",
      " [-3 -2  2 -2  1 -1 -4  1 -1  1 -4  0 -3  1 -2  2]\n",
      " [ 0 -1 -1 -3  1  1 -4 -3 -3 -3 -4  0  1  2 -2 -1]\n",
      " [-3 -1  0  1 -4  1 -1  0  2  1  0 -1  2  0 -1  0]\n",
      " [-2  0 -3 -1 -3 -4  2  1  0  2 -4  1  1 -4 -3  2]\n",
      " [-3 -4  1 -1  0 -1  0 -4 -2 -2  2 -2  2  2 -1  1]\n",
      " [ 0 -1  2  2 -3  0  0 -4 -3 -4 -1 -1 -3  2 -1 -4]\n",
      " [ 1  2 -4  0  0  0 -1  1  1 -1 -1 -3  0  2  1  2]\n",
      " [-2 -2  1  0 -1 -2  2  0 -4 -4  2 -3 -4  1 -4 -3]\n",
      " [-3  2  0 -4 -3  0 -1  1 -4  0  0  2 -4 -1 -3 -4]\n",
      " [ 2 -1  1 -2 -4 -2  0 -3  1 -4 -4  1 -1  2  0 -3]\n",
      " [-1 -3  0 -1 -3  1  2 -3 -1  0 -4 -3 -1 -4  2  1]\n",
      " [-1 -1 -4 -1 -4 -3  1 -1 -4  1  0  0 -3 -3 -3  1]]\n",
      "First Block Weight Matrix ( 16 x 16 ) :\n",
      " [[ 1 -2  0  0 -2  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 2 -4 -1 -4 -1 -3  0  0  0  0  0  0  0  0  0  0]\n",
      " [-2 -4  1 -4 -2  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [-2  1 -3 -2  1 -2  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 1  0  2 -4  1  2  0  0  0  0  0  0  0  0  0  0]\n",
      " [-4 -4 -4  2  1  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [-2 -1 -3 -1 -1 -3  0  0  0  0  0  0  0  0  0  0]\n",
      " [-3  0  0 -1 -1  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [-2 -1  2 -3 -1  1  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  2  0  1  1  2  0  0  0  0  0  0  0  0  0  0]\n",
      " [-1 -1  0 -3  0  1  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 -2 -4 -3 -1 -3  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 2  2  0  1 -4  2  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 2 -4 -1  0 -2 -3  0  0  0  0  0  0  0  0  0  0]\n",
      " [-4  2 -3 -3  2 -2  0  0  0  0  0  0  0  0  0  0]\n",
      " [-1 -3 -1  1 -4 -3  0  0  0  0  0  0  0  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "# ----------------------\n",
    "# Splitting the matrices into 16 x 16 matrices and displaying the first block (for each matrix A & B) that would be obtained using `matrix_split.py`\n",
    "\n",
    "# Block Input Matrices (Ai) (16 x 16)\n",
    "block_input_matrix, input_block_col = matrix_split.matrix_splitting(input_matrix_padded)\n",
    "\n",
    "# Block Weight Matrices (Bi) (16 x 16)\n",
    "block_weight_matrix, weight_block_col = matrix_split.matrix_splitting(weight_matrix_padded)\n",
    "\n",
    "print(\"First Block Input Matrix (\",block_input_matrix[0].shape[0], \"x\", block_input_matrix[0].shape[1],\") :\\n\", block_input_matrix[0])\n",
    "print(\"First Block Weight Matrix (\",block_weight_matrix[0].shape[0], \"x\", block_weight_matrix[0].shape[1],\") :\\n\", block_weight_matrix[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Block of Expected Output Matrix ( 16 x 16 ) :\n",
      " [[ 35  74  17  62   8  23   0   0   0   0   0   0   0   0   0   0]\n",
      " [ 24   2  12  45 -11  24   0   0   0   0   0   0   0   0   0   0]\n",
      " [ 35  19  25  41  -4  17   0   0   0   0   0   0   0   0   0   0]\n",
      " [ 16   9 -14  69   4  18   0   0   0   0   0   0   0   0   0   0]\n",
      " [ 38   6  50  44  21  14   0   0   0   0   0   0   0   0   0   0]\n",
      " [ 60  -8  18  72  16  32   0   0   0   0   0   0   0   0   0   0]\n",
      " [ -2  40   5  54  33  22   0   0   0   0   0   0   0   0   0   0]\n",
      " [ 17  46  -2  72   9  28   0   0   0   0   0   0   0   0   0   0]\n",
      " [ 34   8  21  66  14  43   0   0   0   0   0   0   0   0   0   0]\n",
      " [  4 -14 -11  11  21  -7   0   0   0   0   0   0   0   0   0   0]\n",
      " [ 11  -8   2  12  -9 -10   0   0   0   0   0   0   0   0   0   0]\n",
      " [ 20  18  31  27  36   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [ 22  -3   1  45  44   4   0   0   0   0   0   0   0   0   0   0]\n",
      " [ 25  22  -3  35  25  -1   0   0   0   0   0   0   0   0   0   0]\n",
      " [-18  35 -14  77  26  21   0   0   0   0   0   0   0   0   0   0]\n",
      " [ 35  60  -3  82  47  28   0   0   0   0   0   0   0   0   0   0]]\n"
     ]
    }
   ],
   "source": [
    "# ----------------------\n",
    "# The (16 x 16) block matrices we've obtained are then multiplied using the VTA (GeMM)\n",
    "\n",
    "block_output_matrix, combinations = matrix_multiplication.block_matrix_multiply(block_input_matrix, block_weight_matrix, input_block_col, weight_block_col)\n",
    "print(\"First Block of Expected Output Matrix (\",block_output_matrix[0].shape[0], \"x\", block_output_matrix[0].shape[1],\") :\\n\", block_output_matrix[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Using the example of LeNet-5 first convolutional layer C1 :'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Using the example of LeNet-5 first convolutional layer C1 :\"\"\"\n",
    "\n",
    "# If `doWriteBinaryFile=True`, running this command will generate the binary files containing the data for the A block matrices, transposed B block matrices (input data) and expected output (ACC).\n",
    "# The files will be generated in compiler_output/, under the names 'input.bin', 'weight.bin', expected_out.bin'.\n",
    "\n",
    "# If `doWriteJSON=True`, running this command will also generate the JSON files containing the instructions, UOPs (added later), A block matrices, transposed B block matrices (input data) and expected output (ACC).\n",
    "# The file will be generated in compiler_output/, under the name 'generated_for_compute.json'.\n",
    "\n",
    "#%run ../compiler/data_definition/main_matrix_generator.py examples.data_lenet5_layer1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*How to use operations_definition :*\n",
    "\n",
    "**Objective :** To use the VTA simulators, instructions are to be generated (in .json and .bin files) so they can be run using Scala and/or CHISEL. The following programs use the data obtained from 'data_definition' to generate the instructions for each operation (load, GeMM, ReLU, ALU, store, reset, etc) the VTA needs to perform.\n",
    "\n",
    "**Generating the instructions :** \n",
    "On the example of LeNet-5's first convolutional layer (GeMM), followed by ReLU and average pooling (the aim is to reduce the size of the output matrix after GeMM) :\n",
    "Currently, we have (16 x 16) block INP matrices *Ai*, and (16 x 16) WGT matrices *Bi*. To execute **GeMM**, *Ai* has to be split into (16 x 1) horizontal vectors, to obtain the block OUTPUT matrices *ACCi*, composed of (16 x 1) horizontal vectors. The matrices are reassembled into (16 x 16) blocks.\n",
    "We then apply **ReLU** to *ACCi*. The block matrices we've obtained can now receive the Average Pooling, composed of **2 ADD** and **1 SHR** (data storage divided by 4). \n",
    "\n",
    "For operation (reset, GEMM, ALU), the UOP buffers (`VTAUop` architecture, according to `structures_insn_uop.py`) have to be filled, to determine on which indexes of the matrices the data will be stored / read. \n",
    "The instruction buffers' fields (`VTAGemInsn` for GeMM instructions, `VTAAluInsm` for ALU instructions, `VTAMemInsn` for store / load instructions) should also be input according to the dimensions of the Input tensor and filters.\n",
    "\n",
    "- **LOAD (128-bit):** Data is extracted from DRAM and stored temporarily in SRAM to compute the operations\n",
    "- **ALU (128-bit) :** ReLU activation (for each value x of the matrices => max(0, x)), ADD 1 & 2 then SHR (averaging some of the data).\n",
    "- **GEMM (128-bit) :** Matrix multiplication of A and B (transposed).\n",
    "- **STORE (128-bit) :** Data from SRAM is copied at the desired location in DRAM, after all the operations have been resolved.\n",
    "\n",
    "*Using the program `insn_lenet5_layer1.py` as an example on how to generate the data in binary and JSON files :*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CONFIGURATION\n",
    "\n",
    "Initializing the buffers and importing the necessary structures from `structures_insn_uop.py`.\n",
    "The buffers are used to store the data :\n",
    "- **UOP buffers** : this is where the indexes for each instruction are stored. A UOP buffer has three fields : ACC, INP, WGT. The indexes are the initial indexes upon which the operations will be executed.\n",
    "- **Instruction buffers** : memory storage to describe each instruction. The buffers have different structures depending on whether they're used for memory interactions, or for operation definitions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"CONFIGURATION\"\"\"\n",
    "\n",
    "# PACKAGE IMPORT\n",
    "# --------------\n",
    "import os\n",
    "\n",
    "# Parent folder\n",
    "sys.path.append('../compiler/operations_definition')\n",
    "import structures_insn_uop\n",
    "#sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n",
    "\n",
    "# UOP DEFINITION\n",
    "# --------------\n",
    "# Define empty UOP buffer\n",
    "uop_buffer = []\n",
    "\n",
    "# INSTRUCTION DEFINITION\n",
    "# ----------------------\n",
    "# Define empty instruction buffer\n",
    "insn_buffer = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LOAD DATA FROM DRAM\n",
    "\n",
    "This first buffer is used to reset ACC, before GEMM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (len(uop_buffer) < 1):\n",
    "    uop_buffer.append(structures_insn_uop.VTAUop( # UOP 0 - reset\n",
    "        dst_idx=0, \n",
    "        src_idx=0,\n",
    "        wgt_idx=0\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following buffers mimick memory interactions. \n",
    "Their different fields are :\n",
    "- `opcode` : indicates which operation the buffer defines\n",
    "- DEP FLAG : `pop_dep` asks for permission to send the next instruction, `push_dep` gives the permission. This is to make sure there are no overlaps while data processing.\n",
    "- `buffer_id` : indicates what kind of data is being read\n",
    "- `sram_base`, `dram_base` : sram is the local memory address, where the data is copied from dram, in the case of LOAD\n",
    "- `y_size`, `x_size`, etc : the memory size occupied by the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (len(insn_buffer) < 1):\n",
    "    \n",
    "# Loading the RESET UOP\n",
    "\n",
    "    insn_buffer.append(structures_insn_uop.VTAMemInsn( # I0: LOAD UOP\n",
    "        opcode=0, # 0-LOAD, 1-STORE, 3-FINISH\n",
    "        # DEP FLAG\n",
    "        pop_prev_dep=0,\n",
    "        pop_next_dep=0,\n",
    "        push_prev_dep=0,\n",
    "        push_next_dep=0,\n",
    "        # Memory interaction\n",
    "        buffer_id=0, # 0-UOP, 1-WGT, 2-INP, 3-ACC, 4-OUT, 5-ACC8bit\n",
    "        sram_base=0x0000,\n",
    "        dram_base=0x00001000,\n",
    "        unused=0, # UNUSED\n",
    "        # Operation over the data\n",
    "        y_size=1,\n",
    "        x_size=1,\n",
    "        x_stride=1,\n",
    "        y_pad_top=0,\n",
    "        y_pad_bottom=0,\n",
    "        x_pad_left=0,\n",
    "        x_pad_right=0\n",
    "    ))\n",
    "\n",
    "# The ACC matrix is wiped, in case of RESET\n",
    "\n",
    "    insn_buffer.append(structures_insn_uop.VTAGemInsn( # I1: GEMM RESET\n",
    "        opcode=2, # 2-GEMM\n",
    "        # DEP FLAG\n",
    "        pop_prev_dep=0,\n",
    "        pop_next_dep=0,\n",
    "        push_prev_dep=1, # Ready signal to LOAD\n",
    "        push_next_dep=0,\n",
    "        # Operations\n",
    "        reset=1, # 0-no, 1-reset\n",
    "        uop_bgn=0, # UOP 0\n",
    "        uop_end=1,\n",
    "        loop_out=49, # Number of (16 x 16) blocks in ACC\n",
    "        loop_in=16,  # Block size\n",
    "        # UNUSED\n",
    "        unused=0, # UNUSED\n",
    "        # Index factors\n",
    "        dst_factor_out=16, # Block size\n",
    "        dst_factor_in=1,\n",
    "        src_factor_out=0,\n",
    "        src_factor_in=0,\n",
    "        wgt_factor_out=0,\n",
    "        wgt_factor_in=0\n",
    "    ))\n",
    "    \n",
    "# Loading INP\n",
    "\n",
    "    insn_buffer.append(structures_insn_uop.VTAMemInsn( # I2: LOAD INP\n",
    "        opcode=0, # 0-LOAD, 1-STORE, 3-FINISH\n",
    "        # DEP FLAG\n",
    "        pop_prev_dep=0,\n",
    "        pop_next_dep=1, # Acknowledge COMPUTE ready signal\n",
    "        push_prev_dep=0,\n",
    "        push_next_dep=0,\n",
    "        # Memory interaction\n",
    "        buffer_id=2, # 0-UOP, 1-WGT, 2-INP, 3-ACC, 4-OUT, 5-ACC8bit\n",
    "        sram_base=0x0000,\n",
    "        dram_base=0x00000100,\n",
    "        unused=0, # UNUSED\n",
    "        # Operation over the data\n",
    "        y_size=1,\n",
    "        x_size=1568, # Load 98*16 INP\n",
    "        x_stride=1568,\n",
    "        y_pad_top=0,\n",
    "        y_pad_bottom=0,\n",
    "        x_pad_left=0,\n",
    "        x_pad_right=0\n",
    "    ))\n",
    "    \n",
    "# Loading WGT\n",
    "\n",
    "    insn_buffer.append(structures_insn_uop.VTAMemInsn( # I3: LOAD WGT\n",
    "        opcode=0, # 0-LOAD, 1-STORE, 3-FINISH\n",
    "        # DEP FLAG\n",
    "        pop_prev_dep=0,\n",
    "        pop_next_dep=0,\n",
    "        push_prev_dep=0,\n",
    "        push_next_dep=1, # Ready signal to COMPUTE\n",
    "        # Memory interaction\n",
    "        buffer_id=1, # 0-UOP, 1-WGT, 2-INP, 3-ACC, 4-OUT, 5-ACC8bit\n",
    "        sram_base=0x0000,\n",
    "        dram_base=0x00000020,\n",
    "        unused=0, # UNUSED\n",
    "        # Operation over the data\n",
    "        y_size=1,\n",
    "        x_size=2, # Load 2 WGT\n",
    "        x_stride=2,\n",
    "        y_pad_top=0,\n",
    "        y_pad_bottom=0,\n",
    "        x_pad_left=0,\n",
    "        x_pad_right=0\n",
    "    ))\n",
    "    \n",
    "# Loading UOPs for GEMM & Average Pooling operations\n",
    "\n",
    "    insn_buffer.append(structures_insn_uop.VTAMemInsn( # I4: LOAD UOP\n",
    "        opcode=0, # 0-LOAD, 1-STORE, 3-FINISH\n",
    "        # DEP FLAG\n",
    "        pop_prev_dep=1, # Acknowledge LOAD ready signal\n",
    "        pop_next_dep=0, \n",
    "        push_prev_dep=0,\n",
    "        push_next_dep=0,\n",
    "        # Memory interaction\n",
    "        buffer_id=0, # 0-UOP, 1-WGT, 2-INP, 3-ACC, 4-OUT, 5-ACC8bit\n",
    "        sram_base=0x0001,\n",
    "        dram_base=0x00001001,\n",
    "        unused=0, # UNUSED\n",
    "        # Operation over the data\n",
    "        y_size=1,\n",
    "        x_size=6, # Load 6 UOP (2 GeMM + 1 ReLU + 3 Pool)\n",
    "        x_stride=6,\n",
    "        y_pad_top=0,\n",
    "        y_pad_bottom=0,\n",
    "        x_pad_left=0,\n",
    "        x_pad_right=0\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*HARDWARE REQUIREMENT*\n",
    "\n",
    "The VTA requires, for Compute, (16 x 16) WGT matrices and (16 x 1) vectors for the rest of the data. \n",
    "\n",
    "The (16 x 16) INP and ACC matrices should be split in vectors : the following operations are done on vectors.\n",
    "\n",
    "What follows is an example of a GEMM operation, as operated by the VTA :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First vector of first block of INP matrix ( 16  x  1 )\n",
      "[-1 -2 -2 -3 -3 -3 -2 -4 -3  2 -4 -1  2 -3  1  2] A@0\n",
      "x \n",
      "First block of WGT matrix ( 16  x  16 )\n",
      "[[ 1 -2  0  0 -2  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 2 -4 -1 -4 -1 -3  0  0  0  0  0  0  0  0  0  0]\n",
      " [-2 -4  1 -4 -2  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [-2  1 -3 -2  1 -2  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 1  0  2 -4  1  2  0  0  0  0  0  0  0  0  0  0]\n",
      " [-4 -4 -4  2  1  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [-2 -1 -3 -1 -1 -3  0  0  0  0  0  0  0  0  0  0]\n",
      " [-3  0  0 -1 -1  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [-2 -1  2 -3 -1  1  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  2  0  1  1  2  0  0  0  0  0  0  0  0  0  0]\n",
      " [-1 -1  0 -3  0  1  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 -2 -4 -3 -1 -3  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 2  2  0  1 -4  2  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 2 -4 -1  0 -2 -3  0  0  0  0  0  0  0  0  0  0]\n",
      " [-4  2 -3 -3  2 -2  0  0  0  0  0  0  0  0  0  0]\n",
      " [-1 -3 -1  1 -4 -3  0  0  0  0  0  0  0  0  0  0]] B@0\n",
      "= \n",
      "First vector of first block of ACC ( 16  x  1 )\n",
      "[35 74 17 62  8 23  0  0  0  0  0  0  0  0  0  0] C@0\n"
     ]
    }
   ],
   "source": [
    "# ----------------------\n",
    "# Splitting (16 x 16) block INP matrices into (16 x 1) vectors\n",
    "\n",
    "# Input Matrix INP\n",
    "print(\"First vector of first block of INP matrix (\", np.shape(block_input_matrix[0][0])[0], \" x \", 1, \")\")\n",
    "print(block_input_matrix[0][0], \"A@0\")\n",
    "\n",
    "# Weight Matrix WGT\n",
    "print(\"x \\nFirst block of WGT matrix (\", block_weight_matrix[0].shape[0], \" x \", block_weight_matrix[0].shape[1], \")\")\n",
    "print(block_weight_matrix[0], \"B@0\")\n",
    "\n",
    "# Output Matrix ACC\n",
    "print(\"= \\nFirst vector of first block of ACC (\", np.shape(block_output_matrix[0][0])[0], \" x \", 1, \")\")\n",
    "print(block_output_matrix[0][0], \"C@0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*OPERATION BUFFERS*\n",
    "\n",
    "The following buffers give the structure of each operation. \n",
    "Their fields are :\n",
    "- `opcode` : indicates which operation the buffer defines\n",
    "- DEP FLAG : `pop_dep` asks for permission to send the next instruction, `push_dep` gives the permission. This is to make sure there are no overlaps while data processing.\n",
    "- `reset` : if 'yes', then the stored data is deleted\n",
    "- `uop_bgn`, `uop_end` : the UOP indexes needed for the instruction\n",
    "- `loop_out` : external loops, number of iterations for the operation\n",
    "- `loop_in` : internal loops\n",
    "- `dst_factor_out`, `dst_factor_in` : respectively, incrementation of indexes after each `loop_out`, `loop_in` in ACC\n",
    "- `src_factor_out`, `src_factor_in` : likewise, but in INP\n",
    "- `wgt_factor_out`, `wgt_factor_in` : likewise, but in WGT\n",
    "\n",
    "### GEMM \n",
    "\n",
    "'Generalized Matrix Multiplication' operation. Instead of having block multiplications, all of the blocks of INP should be stacked into a (1568 x 16) matrix then vectorized. This also results in a (784 x 16) ACC matrix, as we have two WGT block matrices (16 x 16).\n",
    "\n",
    "The operation operated by the VTA is as follows : each vector of INP is multiplied with WGT transposed (line by line). The results are stored in ACC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GEMM UOP BUFFER\n",
      "ACC  INP  WGT\n",
      "\n",
      "0    0    0 \n",
      "\n",
      "0    16    1 \n",
      "\n",
      "GEMM INSTRUCTIONS\n",
      "LP_OUT  LP_IN  DST_OUT  DST_IN  SRC_OUT  SRC_IN  WGT_OUT  WGT_IN\n",
      "\n",
      "49       16       16       1       32       1       0       0 \n",
      "\n",
      "ACC - Output matrix post-GEMM ( 784 x 16 )\n",
      "[[ 35.  74.  17. ...   0.   0.   0.]\n",
      " [ 24.   2.  12. ...   0.   0.   0.]\n",
      " [ 35.  19.  25. ...   0.   0.   0.]\n",
      " ...\n",
      " [ 18.  23.  -4. ...   0.   0.   0.]\n",
      " [ 10.  12. -18. ...   0.   0.   0.]\n",
      " [ 47.  28.  36. ...   0.   0.   0.]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"GEMM\"\"\"\n",
    "\n",
    "# Generating the instructions for the GeMM, using A vectorized and B.\n",
    "\n",
    "# ----------------------\n",
    "# Defining the GEMM UOP buffer\n",
    "\n",
    "if (len(uop_buffer) < 1 + 1):\n",
    "    uop_buffer.append(structures_insn_uop.VTAUop( # UOP 1 - GEMM 0\n",
    "        dst_idx=0, \n",
    "        src_idx=0,\n",
    "        wgt_idx=0\n",
    "    ))\n",
    "\n",
    "if (len(uop_buffer) < 2 + 1):\n",
    "    uop_buffer.append(structures_insn_uop.VTAUop( # UOP 2 - GEMM 1\n",
    "        dst_idx=0, \n",
    "        src_idx=16,\n",
    "        wgt_idx=1\n",
    "    ))\n",
    "\n",
    "# ----------------------\n",
    "# Defining the GEMM Instruction buffer\n",
    "\n",
    "index_insn = 5 # Instruction index\n",
    "\n",
    "if (len(insn_buffer) < index_insn + 1):\n",
    "    insn_buffer.append(structures_insn_uop.VTAGemInsn( # I5: GEMM\n",
    "        opcode=2, # 2-GEMM\n",
    "        # DEP FLAG\n",
    "        pop_prev_dep=0,\n",
    "        pop_next_dep=0,\n",
    "        push_prev_dep=0,\n",
    "        push_next_dep=0, \n",
    "        # Operations\n",
    "        reset=0, # 0-no, 1-reset\n",
    "        uop_bgn=1, # UOP 1 + UOP 2\n",
    "        uop_end=3,\n",
    "        loop_out=49,\n",
    "        loop_in=16,\n",
    "        # UNUSED\n",
    "        unused=0, # UNUSED\n",
    "        # Index factors\n",
    "        dst_factor_out=16,\n",
    "        dst_factor_in=1,\n",
    "        src_factor_out=32,\n",
    "        src_factor_in=1,\n",
    "        wgt_factor_out=0,\n",
    "        wgt_factor_in=0\n",
    "    ))\n",
    "\n",
    "# ----------------------\n",
    "# Print the buffers\n",
    " \n",
    "# Printing UOP Buffer\n",
    "def print_uop_buffer(OP, uop_bgn, uop_end) :\n",
    "    print(OP, \"UOP BUFFER\\nACC  INP  WGT\\n\")\n",
    "    for i in range(uop_bgn, uop_end):\n",
    "        print(uop_buffer[i].dst_idx, \"  \", uop_buffer[i].src_idx, \"  \", uop_buffer[i].wgt_idx, \"\\n\")\n",
    "\n",
    "# Printing ALU Instruction Buffer      \n",
    "def print_insn_buffer_ALU(n_insn, OP):\n",
    "    print(OP, \"INSTRUCTIONS\\nLP_OUT  LP_IN  DST_OUT  DST_IN  SRC_OUT  SRC_IN  OPCODE  IMM\\n\")\n",
    "    print(insn_buffer[n_insn].loop_out, \"     \", insn_buffer[n_insn].loop_in, \"     \", insn_buffer[n_insn].dst_factor_out, \"     \", insn_buffer[n_insn].dst_factor_in, \"     \", \n",
    "          insn_buffer[n_insn].src_factor_out, \"     \", \n",
    "          insn_buffer[n_insn].src_factor_in, \"     \", insn_buffer[n_insn].opcode, \"    \", insn_buffer[n_insn].imm)\n",
    "    \n",
    "# ----------------------\n",
    "# Defining GEMM operation\n",
    "\n",
    "def GEMM(A, B):\n",
    "#    assert(A.shape[1] == B.shape[0])\n",
    "    A = np.array(A)\n",
    "    B = np.array(B)\n",
    "    return A @ B\n",
    "\n",
    "# ----------------------\n",
    "# Pseudo-code GEMM\n",
    "\n",
    "def insn_GEMM(ACC, WGT, INP):\n",
    "    for i0 in range(insn_buffer[index_insn].loop_in):\n",
    "        for i1 in range(insn_buffer[index_insn].loop_out):\n",
    "            for uop_index in range(insn_buffer[index_insn].uop_bgn, insn_buffer[index_insn].uop_end):\n",
    "                X, Y, Z = uop_buffer[uop_index].dst_idx, uop_buffer[uop_index].src_idx, uop_buffer[uop_index].wgt_idx\n",
    "                dst_idx = i0 * insn_buffer[index_insn].dst_factor_in + i1 * insn_buffer[index_insn].dst_factor_out + X # Index ACC\n",
    "                inp_idx = i0 * insn_buffer[index_insn].src_factor_in + i1 * insn_buffer[index_insn].src_factor_out + Y # Index INP\n",
    "                wgt_idx = i0 * insn_buffer[index_insn].wgt_factor_in + i1 * insn_buffer[index_insn].wgt_factor_out + Z # Index WGT\n",
    "                ACC[dst_idx] += GEMM(INP[inp_idx], WGT[wgt_idx])                                                       # Storage of GEMM(A, B) in ACC\n",
    "    return ACC\n",
    "\n",
    "# ----------------------\n",
    "# Printing the data\n",
    "# ----------------------\n",
    "\n",
    "# Printing GEMM UOP Buffer\n",
    "print_uop_buffer(\"GEMM\", insn_buffer[index_insn].uop_bgn, insn_buffer[index_insn].uop_end)\n",
    "\n",
    "# Printing GEMM Instruction Buffer \n",
    "print(\"GEMM INSTRUCTIONS\\nLP_OUT  LP_IN  DST_OUT  DST_IN  SRC_OUT  SRC_IN  WGT_OUT  WGT_IN\\n\")\n",
    "print(insn_buffer[index_insn].loop_out, \"     \", insn_buffer[index_insn].loop_in, \"     \", insn_buffer[index_insn].dst_factor_out, \"     \", insn_buffer[index_insn].dst_factor_in, \"     \", \n",
    "        insn_buffer[index_insn].src_factor_out, \"     \", \n",
    "        insn_buffer[index_insn].src_factor_in, \"     \", insn_buffer[index_insn].wgt_factor_out, \"     \", insn_buffer[index_insn].wgt_factor_in, \"\\n\")\n",
    "\n",
    "# Printing the Output Matrix\n",
    "INP_stack = np.vstack(block_input_matrix)       # Stacking the 98 (16 x 16) blocks of A\n",
    "ACC = np.zeros((inp_height, block_size))        # Initializing the Output Matrix C (49 blocks of size (16 x 16) stacked) with zeros\n",
    "\n",
    "ACC_GEMM = insn_GEMM(ACC, block_weight_matrix, INP_stack)\n",
    "#assert(ACC_GEMM[0] == block_output_matrix[0][0])\n",
    "print(\"ACC - Output matrix post-GEMM (\", ACC_GEMM.shape[0], \"x\", ACC_GEMM.shape[1], \")\")\n",
    "print(ACC_GEMM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReLU ACTIVATION\n",
    "\n",
    "ALU operation that, for each value of ACC, returns the maximum value between 0 and the value in ACC. This aims to ensure there are no negative values in ACC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RELU UOP BUFFER\n",
      "ACC  INP  WGT\n",
      "\n",
      "0    0    0 \n",
      "\n",
      "RELU INSTRUCTIONS\n",
      "LP_OUT  LP_IN  DST_OUT  DST_IN  SRC_OUT  SRC_IN  OPCODE  IMM\n",
      "\n",
      "49       16       16       1       16       1       4      0\n",
      "\n",
      "ACC - Output matrix post-ReLU ( 784 x 16 )\n",
      "[[35. 74. 17. ...  0.  0.  0.]\n",
      " [24.  2. 12. ...  0.  0.  0.]\n",
      " [35. 19. 25. ...  0.  0.  0.]\n",
      " ...\n",
      " [18. 23.  0. ...  0.  0.  0.]\n",
      " [10. 12.  0. ...  0.  0.  0.]\n",
      " [47. 28. 36. ...  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"ReLU ACTIVATION\"\"\"\n",
    "\n",
    "# In data_definitions/user_configuration.py, if `useReLU=True` :\n",
    "\n",
    "# ----------------------\n",
    "# Defining the ALU-RELU UOP buffer\n",
    "\n",
    "if (len(uop_buffer) < 3 + 1):\n",
    "    uop_buffer.append(structures_insn_uop.VTAUop( # UOP 3 - ALU (relu)\n",
    "        dst_idx=0, \n",
    "        src_idx=0,\n",
    "        wgt_idx=0\n",
    "    ))\n",
    "\n",
    "# ----------------------\n",
    "# Defining the ALU-RELU Instruction buffer\n",
    "\n",
    "index_insn = 6 # Instruction index\n",
    "\n",
    "if (len(insn_buffer) < index_insn + 1):\n",
    "    insn_buffer.append(structures_insn_uop.VTAAluInsn( # I6: ALU - MAX IMM 0 (relu)\n",
    "        opcode=4, # 4-ALU\n",
    "        # DEP FLAG\n",
    "        pop_prev_dep=0,\n",
    "        pop_next_dep=0,\n",
    "        push_prev_dep=0,\n",
    "        push_next_dep=0,\n",
    "        # Operations\n",
    "        reset=0, # 0-no, 1-reset\n",
    "        uop_bgn=3, # UOP 3\n",
    "        uop_end=4,\n",
    "        loop_out=49,\n",
    "        loop_in=16,\n",
    "        # UNUSED\n",
    "        unused=0, # UNUSED\n",
    "        # Index factors\n",
    "        dst_factor_out=16,\n",
    "        dst_factor_in=1, # ACC incremented by 1\n",
    "        src_factor_out=16,\n",
    "        src_factor_in=1, # INP incremented by 1\n",
    "        alu_opcode=1, # 0-MIN, 1-MAX, 2-ADD, 3-SHR, 4-MUL\n",
    "        use_imm=1, # 0-no, 1-yes\n",
    "        imm=0\n",
    "    ))\n",
    "\n",
    "# ----------------------\n",
    "# Defining RELU operation\n",
    "def RELU(A):\n",
    "    if (useReLU):\n",
    "        A = np.maximum(A, 0)\n",
    "    return A\n",
    "\n",
    "# ----------------------\n",
    "# Pseudo-code ALU RELU\n",
    "\n",
    "def insn_RELU(ACC):\n",
    "    for i0 in range(insn_buffer[index_insn].loop_in):\n",
    "        for i1 in range(insn_buffer[index_insn].loop_out):\n",
    "            for uop_index in range(insn_buffer[index_insn].uop_bgn, insn_buffer[index_insn].uop_end):\n",
    "                X = uop_buffer[uop_index].dst_idx\n",
    "                dst_idx = i0 * insn_buffer[index_insn].dst_factor_in + i1 * insn_buffer[index_insn].dst_factor_out + X # Index ACC\n",
    "                ACC[dst_idx] = RELU(ACC[dst_idx]) # For every row of ACC, we do max(0, value) for each value of the row\n",
    "    return ACC\n",
    "\n",
    "# ----------------------\n",
    "# Printing the data\n",
    "# ----------------------\n",
    "\n",
    "# Printing ReLU UOP Buffer\n",
    "print_uop_buffer(\"RELU\", insn_buffer[index_insn].uop_bgn, insn_buffer[index_insn].uop_end)\n",
    "\n",
    "# Printing ReLU Instruction Buffer \n",
    "print_insn_buffer_ALU(index_insn, \"RELU\")\n",
    "\n",
    "# Printing the Output Matrix\n",
    "\n",
    "ACC_ReLU = insn_RELU(ACC_GEMM)\n",
    "print(\"\\nACC - Output matrix post-ReLU (\", ACC_ReLU.shape[0], \"x\", ACC_ReLU.shape[1], \")\")\n",
    "print(ACC_ReLU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AVERAGE POOLING\n",
    "##### *First ADD*\n",
    "\n",
    "ALU operation that adds two vectors of ACC and stores the result in the first vector of the addition. \n",
    "\n",
    "We first define the UOP buffer, then the instruction buffer. The operation is then computed on ACC and printed out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADD #1 UOP BUFFER\n",
      "ACC  INP  WGT\n",
      "\n",
      "0    1    0 \n",
      "\n",
      "ADD #1 INSTRUCTIONS\n",
      "LP_OUT  LP_IN  DST_OUT  DST_IN  SRC_OUT  SRC_IN  OPCODE  IMM\n",
      "\n",
      "1       392       0       2       0       2       4      0\n",
      "\n",
      "ACC - Output matrix post-first ADD ( 784 x 16 )\n",
      "[[59. 76. 29. ...  0.  0.  0.]\n",
      " [24.  2. 12. ...  0.  0.  0.]\n",
      " [51. 28. 25. ...  0.  0.  0.]\n",
      " ...\n",
      " [18. 23.  0. ...  0.  0.  0.]\n",
      " [57. 40. 36. ...  0.  0.  0.]\n",
      " [47. 28. 36. ...  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"AVERAGE POOLING - First ADD\"\"\"\n",
    "\n",
    "# After this step, the relevant data storage is divided by two.\n",
    "\n",
    "# ----------------------\n",
    "# Defining the ADD #1 UOP buffer\n",
    "\n",
    "if (len(uop_buffer) < 4 + 1):\n",
    "    uop_buffer.append(structures_insn_uop.VTAUop( # UOP 4 - ALU (first add)\n",
    "        dst_idx=0, \n",
    "        src_idx=1,\n",
    "        wgt_idx=0\n",
    "    ))\n",
    "\n",
    "# ----------------------\n",
    "# Defining the ADD #1 Instruction buffer\n",
    "\n",
    "index_insn = 7 # Instruction index\n",
    "\n",
    "if (len(insn_buffer) < index_insn + 1):\n",
    "    insn_buffer.append(structures_insn_uop.VTAAluInsn( # I7: ALU - ADD (Average Pooling 1/3)\n",
    "        opcode=4, # 4-ALU\n",
    "        # DEP FLAG\n",
    "        pop_prev_dep=0,\n",
    "        pop_next_dep=0,\n",
    "        push_prev_dep=0,\n",
    "        push_next_dep=0,\n",
    "        # Operations\n",
    "        reset=0, # 0-no, 1-reset\n",
    "        uop_bgn=4, # UOP 4\n",
    "        uop_end=5,\n",
    "        loop_out=1,\n",
    "        loop_in=392,\n",
    "        # UNUSED\n",
    "        unused=0, # UNUSED\n",
    "        # Index factors\n",
    "        dst_factor_out=0,\n",
    "        dst_factor_in=2, \n",
    "        src_factor_out=0,\n",
    "        src_factor_in=2, \n",
    "        alu_opcode=2, # 0-MIN, 1-MAX, 2-ADD, 3-SHR, 4-MUL\n",
    "        use_imm=0, # 0-no, 1-yes\n",
    "        imm=0\n",
    "    ))\n",
    "\n",
    "# ----------------------\n",
    "# Define ADD operation\n",
    "\n",
    "def ADD(A, B):\n",
    "    A = np.array(A)\n",
    "    B = np.array(B)\n",
    "    return A + B\n",
    "        \n",
    "# ----------------------\n",
    "# Pseudo-code ALU ADD\n",
    "\n",
    "def insn_ADD(ACC):\n",
    "    for i0 in range(insn_buffer[index_insn].loop_in):\n",
    "        for i1 in range(insn_buffer[index_insn].loop_out):\n",
    "            for uop_index in range(insn_buffer[index_insn].uop_bgn, insn_buffer[index_insn].uop_end):\n",
    "                X, Y = uop_buffer[uop_index].dst_idx, uop_buffer[uop_index].src_idx\n",
    "                dst_idx = i0 * insn_buffer[index_insn].dst_factor_in + i1 * insn_buffer[index_insn].dst_factor_out + X\n",
    "                inp_idx = i0 * insn_buffer[index_insn].src_factor_in + i1 * insn_buffer[index_insn].src_factor_out + Y\n",
    "                ACC[dst_idx] = ADD(ACC[dst_idx], ACC[inp_idx])\n",
    "    return ACC\n",
    "\n",
    "# ----------------------\n",
    "# Printing the data\n",
    "# ----------------------\n",
    "\n",
    "# Printing ADD #1 UOP Buffer\n",
    "print_uop_buffer(\"ADD #1\", insn_buffer[index_insn].uop_bgn, insn_buffer[index_insn].uop_end)\n",
    "\n",
    "# Printing ADD #1 Instruction Buffer \n",
    "print_insn_buffer_ALU(index_insn, \"ADD #1\")\n",
    "\n",
    "# Printing the Output Matrix\n",
    "ACC_ADD1 = insn_ADD(ACC_ReLU)\n",
    "print(\"\\nACC - Output matrix post-first ADD (\", ACC_ADD1.shape[0], \"x\", ACC_ADD1.shape[1], \")\")\n",
    "print(ACC_ADD1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### *Second ADD*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADD #2 UOP BUFFER\n",
      "ACC  INP  WGT\n",
      "\n",
      "0    28    0 \n",
      "\n",
      "ADD #2 INSTRUCTIONS\n",
      "LP_OUT  LP_IN  DST_OUT  DST_IN  SRC_OUT  SRC_IN  OPCODE  IMM\n",
      "\n",
      "14       14       56       2       56       2       4      0\n",
      "\n",
      "ACC - Output matrix post-second ADD ( 784 x 16 )\n",
      "[[122. 128. 115. ...   0.   0.   0.]\n",
      " [ 24.   2.  12. ...   0.   0.   0.]\n",
      " [ 75.  81.  32. ...   0.   0.   0.]\n",
      " ...\n",
      " [ 18.  23.   0. ...   0.   0.   0.]\n",
      " [ 57.  40.  36. ...   0.   0.   0.]\n",
      " [ 47.  28.  36. ...   0.   0.   0.]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"AVERAGE POOLING - Second ADD\"\"\"\n",
    "\n",
    "# After this step, the relevant data storage is divided by two. (4 total)\n",
    "\n",
    "# ----------------------\n",
    "# Defining the ADD #2 UOP buffer\n",
    "\n",
    "if (len(uop_buffer) < 5 + 1):\n",
    "    uop_buffer.append(structures_insn_uop.VTAUop( # UOP 5 - ALU (second add)\n",
    "        dst_idx=0, \n",
    "        src_idx=28,\n",
    "        wgt_idx=0\n",
    "    ))\n",
    "\n",
    "# ----------------------\n",
    "# Defining the ADD #2 Instruction buffer\n",
    "\n",
    "index_insn = 8 # Instruction index\n",
    "\n",
    "if (len(insn_buffer) < index_insn + 1):\n",
    "    insn_buffer.append(structures_insn_uop.VTAAluInsn( # I8: ALU - ADD (Average Pooling 2/3)\n",
    "        opcode=4, # 4-ALU\n",
    "        # DEP FLAG\n",
    "        pop_prev_dep=0,\n",
    "        pop_next_dep=0,\n",
    "        push_prev_dep=0,\n",
    "        push_next_dep=0,\n",
    "        # Operations\n",
    "        reset=0, # 0-no, 1-reset\n",
    "        uop_bgn=5, # UOP 5\n",
    "        uop_end=6,\n",
    "        loop_out=14,\n",
    "        loop_in=14,\n",
    "        # UNUSED\n",
    "        unused=0, # UNUSED\n",
    "        # Index factors\n",
    "        dst_factor_out=56,\n",
    "        dst_factor_in=2, \n",
    "        src_factor_out=56,\n",
    "        src_factor_in=2, \n",
    "        alu_opcode=2, # 0-MIN, 1-MAX, 2-ADD, 3-SHR, 4-MUL\n",
    "        use_imm=0, # 0-no, 1-yes\n",
    "        imm=0\n",
    "    ))\n",
    "\n",
    "# ----------------------\n",
    "# Printing the data\n",
    "# ----------------------\n",
    "\n",
    "# Printing ADD #2 UOP Buffer\n",
    "print_uop_buffer(\"ADD #2\", insn_buffer[index_insn].uop_bgn, insn_buffer[index_insn].uop_end)\n",
    "\n",
    "# Printing ADD #2 Instruction Buffer \n",
    "print_insn_buffer_ALU(index_insn, \"ADD #2\")\n",
    "\n",
    "# Printing the Output Matrix\n",
    "ACC_ADD2 = insn_ADD(ACC_ADD1)\n",
    "print(\"\\nACC - Output matrix post-second ADD (\", ACC_ADD2.shape[0], \"x\", ACC_ADD2.shape[1], \")\")\n",
    "print(ACC_ADD2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHR UOP BUFFER\n",
      "ACC  INP  WGT\n",
      "\n",
      "0    0    0 \n",
      "\n",
      "SHR INSTRUCTIONS\n",
      "LP_OUT  LP_IN  DST_OUT  DST_IN  SRC_OUT  SRC_IN  OPCODE  IMM\n",
      "\n",
      "14       14       56       2       56       2       4      2\n",
      "\n",
      "ACC - Output matrix post-SHR ( 784 x 16 )\n",
      "[[30. 32. 28. ...  0.  0.  0.]\n",
      " [24.  2. 12. ...  0.  0.  0.]\n",
      " [18. 20.  8. ...  0.  0.  0.]\n",
      " ...\n",
      " [18. 23.  0. ...  0.  0.  0.]\n",
      " [57. 40. 36. ...  0.  0.  0.]\n",
      " [47. 28. 36. ...  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"AVERAGE POOLING - SHR\"\"\"\n",
    "\n",
    "# With this step, we average the added values.\n",
    "\n",
    "# ----------------------\n",
    "# Defining the SHR UOP buffer\n",
    "\n",
    "if (len(uop_buffer) < 6 + 1):\n",
    "    uop_buffer.append(structures_insn_uop.VTAUop( # UOP 6 - ALU (shift right)\n",
    "        dst_idx=0, \n",
    "        src_idx=0,\n",
    "        wgt_idx=0\n",
    "    ))\n",
    "\n",
    "# ----------------------\n",
    "# Defining the ALU-SHR Instruction buffer\n",
    "\n",
    "index_insn = 9 # Instruction index\n",
    "\n",
    "if (len(insn_buffer) < index_insn + 1):\n",
    "    insn_buffer.append(structures_insn_uop.VTAAluInsn( # I9: ALU - SHR (Average Pooling 3/3)\n",
    "        opcode=4, # 4-ALU\n",
    "        # DEP FLAG\n",
    "        pop_prev_dep=0,\n",
    "        pop_next_dep=0,\n",
    "        push_prev_dep=0,\n",
    "        push_next_dep=1, # Ready signal to STORE\n",
    "        # Operations\n",
    "        reset=0, # 0-no, 1-reset\n",
    "        uop_bgn=6, # UOP 6\n",
    "        uop_end=7,\n",
    "        loop_out=14,\n",
    "        loop_in=14,\n",
    "        # UNUSED\n",
    "        unused=0, # UNUSED\n",
    "        # Index factors\n",
    "        dst_factor_out=56,\n",
    "        dst_factor_in=2, \n",
    "        src_factor_out=56,\n",
    "        src_factor_in=2, \n",
    "        alu_opcode=3, # 0-MIN, 1-MAX, 2-ADD, 3-SHR, 4-MUL\n",
    "        use_imm=1, # 0-no, 1-yes\n",
    "        imm=2 # Division by 4 (rounded down)\n",
    "    ))\n",
    "\n",
    "# ----------------------\n",
    "# Defining SHR operation\n",
    "\n",
    "def SHR(A, IMM) :\n",
    "    for i in range(len(A)): # A composed of horizontal vectors (16 x 1)\n",
    "        A[i] = int(np.float64(A[i])) >> IMM\n",
    "    return A\n",
    "\n",
    "# ----------------------\n",
    "# Pseudo-code ALU SHR\n",
    "\n",
    "def insn_SHR(ACC):\n",
    "    for i0 in range(insn_buffer[index_insn].loop_in):\n",
    "        for i1 in range(insn_buffer[index_insn].loop_out):\n",
    "            for uop_index in range(insn_buffer[index_insn].uop_bgn, insn_buffer[index_insn].uop_end):\n",
    "                X = uop_buffer[uop_index].dst_idx\n",
    "                dst_idx = i0 * insn_buffer[index_insn].dst_factor_in + i1 * insn_buffer[index_insn].dst_factor_out + X\n",
    "                ACC[dst_idx] = SHR(ACC[dst_idx], insn_buffer[index_insn].imm)\n",
    "    return ACC\n",
    "\n",
    "# ----------------------\n",
    "# Printing the data\n",
    "# ----------------------\n",
    "\n",
    "# Printing SHR UOP Buffer\n",
    "print_uop_buffer(\"SHR\", insn_buffer[index_insn].uop_bgn, insn_buffer[index_insn].uop_end)\n",
    "\n",
    "# Printing SHR Instruction Buffer \n",
    "print_insn_buffer_ALU(index_insn, \"SHR\")\n",
    "\n",
    "# Printing the Output Matrix\n",
    "ACC_SHR = insn_SHR(ACC_ADD2)\n",
    "print(\"\\nACC - Output matrix post-SHR (\", ACC_SHR.shape[0], \"x\", ACC_SHR.shape[1], \")\")\n",
    "print(ACC_SHR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"DATA STORAGE FROM SRAM TO DRAM\"\"\"\n",
    "\n",
    "insn_buffer.append(structures_insn_uop.VTAMemInsn( # I10: STORE\n",
    "    opcode=1, # 0-LOAD, 1-STORE, 3-FINISH\n",
    "    # DEP FLAG\n",
    "    pop_prev_dep=1, # Acknowledge COMPUTE ready signal\n",
    "    pop_next_dep=0,\n",
    "    push_prev_dep=1, # Ready signal to COMPUTE\n",
    "    push_next_dep=0,\n",
    "    # Memory interaction\n",
    "    buffer_id=4, # 0-UOP, 1-WGT, 2-INP, 3-ACC, 4-OUT, 5-ACC8bit\n",
    "    sram_base=0x0000,\n",
    "    dram_base=0x00000300,\n",
    "    unused=0, # UNUSED\n",
    "    # Operation over the data\n",
    "    y_size=1,\n",
    "    x_size=784, # Store 49*16 OUT\n",
    "    x_stride=784,\n",
    "    y_pad_top=0,\n",
    "    y_pad_bottom=0,\n",
    "    x_pad_left=0,\n",
    "    x_pad_right=0\n",
    "))\n",
    "\n",
    "insn_buffer.append(structures_insn_uop.VTAMemInsn( # I11: NOP-MEMORY-STAGE\n",
    "    opcode=0, # 0-LOAD, 1-STORE, 3-FINISH\n",
    "    # DEP FLAG\n",
    "    pop_prev_dep=0,\n",
    "    pop_next_dep=0,\n",
    "    push_prev_dep=0, \n",
    "    push_next_dep=1, # Ready signal to COMPUTE\n",
    "    # Memory interaction\n",
    "    buffer_id=2, # 0-UOP, 1-WGT, 2-INP, 3-ACC, 4-OUT, 5-ACC8bit\n",
    "    sram_base=0x0000,\n",
    "    dram_base=0x00000000,\n",
    "    unused=0, # UNUSED\n",
    "    # Operation over the data\n",
    "    y_size=0,\n",
    "    x_size=0,\n",
    "    x_stride=0,\n",
    "    y_pad_top=0,\n",
    "    y_pad_bottom=0,\n",
    "    x_pad_left=0,\n",
    "    x_pad_right=0\n",
    "))\n",
    "\n",
    "insn_buffer.append(structures_insn_uop.VTAMemInsn( # I12: NOP-COMPUTE-STAGE\n",
    "    opcode=0, # 0-LOAD, 1-STORE, 3-FINISH\n",
    "    # DEP FLAG\n",
    "    pop_prev_dep=1, # Acknowledge LOAD ready signal\n",
    "    pop_next_dep=1, # Acknowledge STORE ready signal\n",
    "    push_prev_dep=0,\n",
    "    push_next_dep=0,\n",
    "    # Memory interaction\n",
    "    buffer_id=0, # 0-UOP, 1-WGT, 2-INP, 3-ACC, 4-OUT, 5-ACC8bit\n",
    "    sram_base=0x0000,\n",
    "    dram_base=0x00000000,\n",
    "    unused=0, # UNUSED\n",
    "    # Operation over the data\n",
    "    y_size=0,\n",
    "    x_size=0,\n",
    "    x_stride=0,\n",
    "    y_pad_top=0,\n",
    "    y_pad_bottom=0,\n",
    "    x_pad_left=0,\n",
    "    x_pad_right=0\n",
    "))\n",
    "\n",
    "insn_buffer.append(structures_insn_uop.VTAMemInsn( # I13: FINISH\n",
    "    opcode=3, # 0-LOAD, 1-STORE, 3-FINISH\n",
    "    # DEP FLAG\n",
    "    pop_prev_dep=0,\n",
    "    pop_next_dep=0,\n",
    "    push_prev_dep=0,\n",
    "    push_next_dep=0,\n",
    "    # Memory interaction\n",
    "    buffer_id=0, # 0-UOP, 1-WGT, 2-INP, 3-ACC, 4-OUT, 5-ACC8bit\n",
    "    sram_base=0x0000,\n",
    "    dram_base=0x00000000,\n",
    "    unused=0, # UNUSED\n",
    "    # Operation over the data\n",
    "    y_size=0,\n",
    "    x_size=0,\n",
    "    x_stride=0,\n",
    "    y_pad_top=0,\n",
    "    y_pad_bottom=0,\n",
    "    x_pad_left=0,\n",
    "    x_pad_right=0\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How to retrieve the encoded data :** \n",
    "\n",
    "Each of the 32-bit UOPs and 128-bit instructions are encoded in hexadecimal pairs. \n",
    "\n",
    "*Example of encoding with the SHR Instruction :*\n",
    "\n",
    "Structure used : \n",
    "\n",
    "```\n",
    "class VTAAluInsn(LittleEndianStructure):\n",
    "    \"\"\"ALU instruction structure (128-bit).\"\"\"\n",
    "    _pack_ = 1\n",
    "    _fields_ = [\n",
    "        (\"opcode\", c_uint64, 3),\n",
    "        (\"pop_prev_dep\", c_uint64, 1),\n",
    "        (\"pop_next_dep\", c_uint64, 1),\n",
    "        (\"push_prev_dep\", c_uint64, 1),\n",
    "        (\"push_next_dep\", c_uint64, 1),\n",
    "        (\"reset\", c_uint64, 1),\n",
    "        (\"uop_bgn\", c_uint64, 13),\n",
    "        (\"uop_end\", c_uint64, 14),\n",
    "        (\"loop_out\", c_uint64, 14),\n",
    "        (\"loop_in\", c_uint64, 14),\n",
    "        (\"unused\", c_uint64, 1),\n",
    "        (\"dst_factor_out\", c_uint64, 11),\n",
    "        (\"dst_factor_in\", c_uint64, 11),\n",
    "        (\"src_factor_out\", c_uint64, 11),\n",
    "        (\"src_factor_in\", c_uint64, 11),\n",
    "        (\"alu_opcode\", c_uint64, 3), # 0-MIN, 1-MAX, 2-ADD, 3-SHR, 4-MUL/SHL\n",
    "        (\"use_imm\", c_uint64, 1), # 0-NO, 1-YES\n",
    "        (\"imm\", c_uint64, 16)\n",
    "    ]\n",
    "```\n",
    "\n",
    "Buffer configuration :\n",
    "\n",
    "```\n",
    "insn_buffer.append(structures_insn_uop.VTAAluInsn( # I9: ALU - SHR (Average Pooling 3/3)\n",
    "        opcode=4,           # >>> 100 [3-bit]\n",
    "        # DEP FLAG          # >>> 0001\n",
    "        pop_prev_dep=0,     \n",
    "        pop_next_dep=0,    \n",
    "        push_prev_dep=0,    \n",
    "        push_next_dep=1,    \n",
    "        # Operations\n",
    "        reset=0,            # >>> 0\n",
    "        uop_bgn=6,          # >>> 0000 0000 0011 0\n",
    "        uop_end=7,          # >>> 0000 0000 0001 11\n",
    "        loop_out=14,        # >>> 0000 0000 0011 10\n",
    "        loop_in=14,         # >>> 0000 0000 0011 10\n",
    "        # UNUSED\n",
    "        unused=0,           # >>> 0 \n",
    "        # Index factors\n",
    "        dst_factor_out=56,  # >>> 0000 0111 000\n",
    "        dst_factor_in=2,    # >>> 0000 0000 010\n",
    "        src_factor_out=56,  # >>> 0000 0111 000\n",
    "        src_factor_in=2,    # >>> 0000 0000 010\n",
    "        alu_opcode=3,       # >>> 011\n",
    "        use_imm=1,          # >>> 1\n",
    "        imm=2               # >>> 0000 0000 0000 0010\n",
    "    ))\n",
    "```\n",
    "\n",
    "==> In hexadecimal, we obtain the following instruction (Big Endian): **I9 : 00 02 b0 04 0e 00 10 38 00 1c 00 70 00 e0 06 44**\n",
    "\n",
    "In the .bin files (Little Endian), we obtain : **I9 : 44 06 e0 00 70 00 1c 00 38 10 00 0e 04 b0 02 00**\n",
    "\n",
    "***BINARY FILES***\n",
    "\n",
    "To obtain the binary files needed for `functional_simulator`, run the following commands in the *OUTPUT/* directory :\n",
    "\n",
    ">> This command will generate the UOP and Instruction files in *OUTPUT/* : `python ../compiler/operations_definition/examples/insn_lenet5_layer1.py`\n",
    "\n",
    ">> To access the data in the files, run the commands :\n",
    "- For the UOP : `hexdump -C uop.bin > uop_letnet5_layer1_bin.txt` \n",
    "- For the instructions : `hexdump -C instructions.bin > insn_lenet5_layer1_bin.txt`\n",
    "\n",
    "The generated binary files can then be copied into *simulators/functional_simulator/binary_input_files/*\n",
    "\n",
    "***JSON FILES***\n",
    "\n",
    "For CHISEL, all of the instructions described above are encoded below :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hexadecimal UOPs for CHISEL simulation (cycle_accurate_simulator)\n",
      "\n",
      "\n",
      "UOP0:\n",
      "0x00000000   \n",
      "\n",
      "UOP1:\n",
      "0x00000000   \n",
      "\n",
      "UOP2:\n",
      "0x00408000   \n",
      "\n",
      "UOP3:\n",
      "0x00000000   \n",
      "\n",
      "UOP4:\n",
      "0x00000800   \n",
      "\n",
      "UOP5:\n",
      "0x0000E000   \n",
      "\n",
      "UOP6:\n",
      "0x00000000   \n",
      "\n",
      "\n",
      "Hexadecimal instructions for CHISEL simulation (cycle_accurate_simulator)\n",
      "\n",
      "\n",
      "I0:\n",
      "0x00000001 00010001 00000040 00000000\n",
      "\n",
      "I1:\n",
      "0x00000000 00000810 00200188 002000A2\n",
      "\n",
      "I2:\n",
      "0x00000620 06200001 00000004 00000110\n",
      "\n",
      "I3:\n",
      "0x00000002 00020001 00000000 800000C0\n",
      "\n",
      "I4:\n",
      "0x00000006 00060001 00000040 04000408\n",
      "\n",
      "I5:\n",
      "0x00000002 08000810 00200188 00600102\n",
      "\n",
      "I6:\n",
      "0x00009002 04000810 00200188 00800304\n",
      "\n",
      "I7:\n",
      "0x00002004 00001000 03100008 00A00404\n",
      "\n",
      "I8:\n",
      "0x00002004 0E001038 001C0070 00C00504\n",
      "\n",
      "I9:\n",
      "0x0002B004 0E001038 001C0070 00E00644\n",
      "\n",
      "I10:\n",
      "0x00000310 03100001 0000000C 00000229\n",
      "\n",
      "I11:\n",
      "0x00000000 00000000 00000000 00000140\n",
      "\n",
      "I12:\n",
      "0x00000000 00000000 00000000 00000018\n",
      "\n",
      "I13:\n",
      "0x00000000 00000000 00000000 00000003\n"
     ]
    }
   ],
   "source": [
    "\"\"\"JSON FILE\"\"\"\n",
    "\n",
    "# Output to be copied in a .json file (in our case : 'generated_for_compute.json').\n",
    "\n",
    "print(\"Hexadecimal UOPs for CHISEL simulation (cycle_accurate_simulator)\\n\")\n",
    "i = 0\n",
    "for uop in uop_buffer:\n",
    "    print(f\"\\nUOP{i}:\")\n",
    "    structures_insn_uop.print_hex_128bit(uop)\n",
    "    i = i + 1\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Hexadecimal instructions for CHISEL simulation (cycle_accurate_simulator)\\n\")\n",
    "i = 0\n",
    "for insn in insn_buffer:\n",
    "    print(f\"\\nI{i}:\")\n",
    "    structures_insn_uop.print_hex_128bit(insn)\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generated .json files can be copied into *simulators/cycle_accurate_simulator/src/test/resources/*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
